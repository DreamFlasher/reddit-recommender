{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "This file handles all preprocessing of the original file 'safe_links_all.gz'\n",
    "\n",
    "1. drop all non-image files\n",
    "2. retrieve comments for posts (currently disabled, due to memory leak) \n",
    "3. downloading corresponding image, associating with hash of url\n",
    "4. converting to fasttext format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-10 13:08:57 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gzip\n",
    "from collections import namedtuple\n",
    "import requests\n",
    "import traceback\n",
    "import hashlib\n",
    "import praw\n",
    "import wget\n",
    "from praw.models import MoreComments\n",
    "import logging\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, stream=sys.stdout)\n",
    "import os\n",
    "import os.path\n",
    "from multiprocessing import Pool\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07 00:15:20 0\n",
      "2019-01-07 00:16:03 1\n",
      "2019-01-07 00:16:45 2\n",
      "2019-01-07 00:17:33 3\n",
      "2019-01-07 00:18:21 4\n",
      "2019-01-07 00:19:10 5\n",
      "2019-01-07 00:20:02 6\n",
      "2019-01-07 00:20:53 7\n",
      "2019-01-07 00:21:48 8\n",
      "2019-01-07 00:22:39 9\n",
      "2019-01-07 00:23:31 10\n",
      "2019-01-07 00:24:20 11\n",
      "2019-01-07 00:25:14 12\n",
      "2019-01-07 00:26:03 13\n",
      "2019-01-07 00:26:53 14\n",
      "2019-01-07 00:27:48 15\n",
      "2019-01-07 00:28:39 16\n",
      "2019-01-07 00:29:29 17\n",
      "2019-01-07 00:30:24 18\n",
      "2019-01-07 00:31:15 19\n",
      "2019-01-07 00:32:06 20\n",
      "2019-01-07 00:32:57 21\n",
      "2019-01-07 00:33:55 22\n",
      "2019-01-07 00:34:45 23\n",
      "2019-01-07 00:35:35 24\n",
      "2019-01-07 00:36:25 25\n",
      "2019-01-07 00:37:15 26\n",
      "2019-01-07 00:38:05 27\n",
      "2019-01-07 00:39:04 28\n",
      "2019-01-07 00:39:54 29\n",
      "2019-01-07 00:40:48 30\n",
      "2019-01-07 00:41:42 31\n",
      "2019-01-07 00:42:36 32\n",
      "2019-01-07 00:43:41 33\n",
      "2019-01-07 00:44:34 34\n",
      "2019-01-07 00:45:28 35\n",
      "2019-01-07 00:46:23 36\n",
      "2019-01-07 00:47:17 37\n",
      "2019-01-07 00:48:12 38\n",
      "2019-01-07 00:49:08 39\n",
      "2019-01-07 00:50:02 40\n",
      "2019-01-07 00:51:10 41\n",
      "2019-01-07 00:52:08 42\n",
      "2019-01-07 00:53:06 43\n",
      "2019-01-07 00:54:03 44\n",
      "2019-01-07 00:54:59 45\n",
      "2019-01-07 00:55:54 46\n",
      "2019-01-07 00:56:48 47\n",
      "2019-01-07 00:57:41 48\n",
      "2019-01-07 00:58:50 49\n",
      "2019-01-07 00:59:43 50\n",
      "2019-01-07 01:00:38 51\n",
      "2019-01-07 01:01:33 52\n",
      "2019-01-07 01:02:30 53\n",
      "2019-01-07 01:03:23 54\n",
      "2019-01-07 01:04:14 55\n",
      "2019-01-07 01:05:06 56\n",
      "2019-01-07 01:05:57 57\n",
      "2019-01-07 01:06:49 58\n",
      "2019-01-07 01:07:41 59\n",
      "2019-01-07 01:08:33 60\n",
      "2019-01-07 01:09:45 61\n",
      "2019-01-07 01:10:37 62\n",
      "2019-01-07 01:11:28 63\n",
      "2019-01-07 01:12:19 64\n",
      "2019-01-07 01:13:11 65\n",
      "2019-01-07 01:14:02 66\n",
      "2019-01-07 01:14:53 67\n",
      "2019-01-07 01:15:43 68\n",
      "2019-01-07 01:16:34 69\n",
      "2019-01-07 01:17:23 70\n",
      "2019-01-07 01:18:11 71\n",
      "2019-01-07 01:18:58 72\n",
      "2019-01-07 01:19:46 73\n",
      "2019-01-07 01:20:36 74\n",
      "2019-01-07 01:21:27 75\n",
      "2019-01-07 01:22:16 76\n",
      "2019-01-07 01:23:30 77\n",
      "2019-01-07 01:24:21 78\n",
      "2019-01-07 01:25:13 79\n",
      "2019-01-07 01:26:08 80\n",
      "2019-01-07 01:26:59 81\n",
      "2019-01-07 01:27:49 82\n",
      "2019-01-07 01:28:39 83\n",
      "2019-01-07 01:29:30 84\n",
      "2019-01-07 01:30:20 85\n",
      "2019-01-07 01:30:59 86\n",
      "2019-01-07 01:31:38 87\n",
      "2019-01-07 01:32:18 88\n",
      "2019-01-07 01:32:56 89\n",
      "2019-01-07 01:33:34 90\n",
      "2019-01-07 01:34:12 91\n",
      "2019-01-07 01:34:49 92\n",
      "2019-01-07 01:35:25 93\n",
      "2019-01-07 01:36:04 94\n",
      "2019-01-07 01:36:43 95\n",
      "2019-01-07 01:37:22 96\n",
      "2019-01-07 01:38:02 97\n",
      "2019-01-07 01:38:42 98\n",
      "2019-01-07 01:39:21 99\n",
      "2019-01-07 01:40:00 100\n",
      "2019-01-07 01:40:38 101\n",
      "2019-01-07 01:41:16 102\n",
      "2019-01-07 01:41:54 103\n",
      "2019-01-07 01:43:02 104\n",
      "2019-01-07 01:43:41 105\n",
      "2019-01-07 01:44:19 106\n",
      "2019-01-07 01:44:56 107\n",
      "2019-01-07 01:45:33 108\n",
      "2019-01-07 01:46:10 109\n",
      "2019-01-07 01:46:47 110\n",
      "2019-01-07 01:47:26 111\n",
      "2019-01-07 01:48:03 112\n",
      "2019-01-07 01:48:39 113\n",
      "2019-01-07 01:49:16 114\n",
      "2019-01-07 01:49:52 115\n",
      "2019-01-07 01:50:27 116\n",
      "2019-01-07 01:51:01 117\n",
      "2019-01-07 01:51:34 118\n",
      "2019-01-07 01:52:11 119\n",
      "2019-01-07 01:52:48 120\n",
      "2019-01-07 01:53:25 121\n",
      "2019-01-07 01:54:00 122\n",
      "2019-01-07 01:54:37 123\n",
      "2019-01-07 01:55:13 124\n",
      "2019-01-07 01:55:49 125\n",
      "2019-01-07 01:56:26 126\n",
      "2019-01-07 01:57:03 127\n",
      "2019-01-07 01:57:40 128\n",
      "2019-01-07 01:58:17 129\n",
      "2019-01-07 01:58:56 130\n",
      "2019-01-07 01:59:37 131\n",
      "2019-01-07 02:00:18 132\n",
      "2019-01-07 02:00:58 133\n",
      "2019-01-07 02:01:40 134\n",
      "2019-01-07 02:02:21 135\n",
      "2019-01-07 02:03:02 136\n",
      "2019-01-07 02:03:43 137\n",
      "2019-01-07 02:04:24 138\n",
      "2019-01-07 02:05:06 139\n",
      "2019-01-07 02:05:46 140\n",
      "2019-01-07 02:06:27 141\n",
      "2019-01-07 02:07:08 142\n",
      "2019-01-07 02:07:49 143\n",
      "2019-01-07 02:08:29 144\n",
      "2019-01-07 02:09:10 145\n",
      "2019-01-07 02:10:29 146\n",
      "2019-01-07 02:11:08 147\n",
      "2019-01-07 02:11:47 148\n",
      "2019-01-07 02:12:26 149\n",
      "2019-01-07 02:13:05 150\n",
      "2019-01-07 02:13:43 151\n",
      "2019-01-07 02:14:21 152\n",
      "2019-01-07 02:15:01 153\n",
      "2019-01-07 02:15:41 154\n",
      "2019-01-07 02:16:21 155\n",
      "2019-01-07 02:17:02 156\n",
      "2019-01-07 02:17:41 157\n",
      "2019-01-07 02:18:22 158\n",
      "2019-01-07 02:19:01 159\n",
      "2019-01-07 02:19:40 160\n",
      "2019-01-07 02:20:19 161\n",
      "2019-01-07 02:21:09 162\n",
      "2019-01-07 02:21:58 163\n",
      "2019-01-07 02:22:47 164\n",
      "2019-01-07 02:23:37 165\n",
      "2019-01-07 02:24:30 166\n",
      "2019-01-07 02:25:25 167\n",
      "2019-01-07 02:26:20 168\n",
      "2019-01-07 02:27:16 169\n",
      "2019-01-07 02:28:11 170\n",
      "2019-01-07 02:29:06 171\n",
      "2019-01-07 02:30:02 172\n",
      "2019-01-07 02:30:59 173\n",
      "2019-01-07 02:31:57 174\n",
      "2019-01-07 02:32:54 175\n",
      "2019-01-07 02:33:52 176\n",
      "2019-01-07 02:34:51 177\n",
      "2019-01-07 02:35:50 178\n",
      "2019-01-07 02:36:48 179\n",
      "2019-01-07 02:38:34 180\n",
      "2019-01-07 02:39:37 181\n",
      "2019-01-07 02:40:42 182\n",
      "2019-01-07 02:41:46 183\n",
      "2019-01-07 02:42:48 184\n",
      "2019-01-07 02:43:50 185\n",
      "2019-01-07 02:44:51 186\n",
      "2019-01-07 02:45:52 187\n",
      "2019-01-07 02:46:54 188\n",
      "2019-01-07 02:47:56 189\n",
      "2019-01-07 02:49:00 190\n",
      "2019-01-07 02:50:03 191\n",
      "2019-01-07 02:51:07 192\n",
      "2019-01-07 02:52:10 193\n",
      "2019-01-07 02:53:12 194\n",
      "2019-01-07 02:54:13 195\n",
      "2019-01-07 02:55:15 196\n",
      "2019-01-07 02:56:18 197\n",
      "2019-01-07 02:57:22 198\n",
      "2019-01-07 02:58:25 199\n",
      "2019-01-07 02:59:29 200\n",
      "2019-01-07 03:00:32 201\n",
      "2019-01-07 03:01:35 202\n",
      "2019-01-07 03:02:37 203\n",
      "2019-01-07 03:03:40 204\n",
      "2019-01-07 03:04:43 205\n",
      "2019-01-07 03:05:47 206\n"
     ]
    }
   ],
   "source": [
    "row = namedtuple('row_raw', ['subreddit', 'submission_title', 'submitted_link', 'comments_link', 'short_name'])\n",
    "base_path = \"/mnt/marcel/\"\n",
    "\n",
    "reddit = praw.Reddit(user_agent='Comment Extraction',\n",
    "         client_id='BQD4SITRawON4Q', client_secret=\"noSAX9aSU3VRPowZnhTytXm8sOk\",\n",
    "         )\n",
    "\n",
    "def retrieve_file(url, filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        try:\n",
    "            wget.download(url, filename, None)\n",
    "        except:\n",
    "            pass \n",
    "\n",
    "def retrieve_comments(link):\n",
    "    try:\n",
    "        comments = reddit.submission(url='https://www.reddit.com'+link).comments.list()\n",
    "        comments_string = \"\"\n",
    "        for comment in comments:\n",
    "            if not isinstance(comment, MoreComments):\n",
    "                comments_string += comment.body + \" \"\n",
    "        with gzip.open(base_path+'comments/%s.gz' % os.getpid(),'at') as f:\n",
    "            f.write(str([link, comments_string])+\"\\n\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "with gzip.open(base_path+'safe_links_all.gz','rt') as f, gzip.open(base_path+'safe_links_imgposts.gz','wt') as fout, Pool(processes=32) as pool:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.replace(',null]', ',\"\"]').replace(',null,', ',\"\",').replace('[null,', '[\"\",')\n",
    "        if i%1000000==0:  # total number of lines: 206669601\n",
    "            logging.info(i//1000000)\n",
    "        line_list = eval(line)  # convert the str(list) to a list, don't do this in production, as this has the danger of code injection\n",
    "        r = row._make(line_list)\n",
    "        submitted_link = r.submitted_link\n",
    "        if '?' in submitted_link:  # crude way to strip url params\n",
    "            submitted_link = submitted_link[:r.submitted_link.find('?', 0)]                \n",
    "\n",
    "        if submitted_link.endswith(\".jpg\") or submitted_link.endswith(\".png\") or submitted_link.endswith(\".jpeg\"):\n",
    "            imgurlhash = hashlib.sha256(r.submitted_link.encode('utf-8')).hexdigest()\n",
    "            suffix = \".jpg\"\n",
    "            if submitted_link.endswith(\".png\"):\n",
    "                suffix = \".png\"\n",
    "            imgurlhash += suffix\n",
    "\n",
    "            filename = base_path+\"img/\"+imgurlhash\n",
    "            pool.apply_async(retrieve_file, (r.submitted_link, filename))\n",
    "            #pool.apply_async(retrieve_comments, (r.comments_link, ))\n",
    "\n",
    "            outlist = list(r)\n",
    "            outlist.append(imgurlhash)                \n",
    "            fout.write(str(outlist)+\"\\n\")\n",
    "            \n",
    "            #with gzip.open(base_path+'subreddits/%s.gz' % r.subreddit,'at') as f:\n",
    "            #    f.write(r.submission_title+\"\\n\")\n",
    "\n",
    "            #outlist.append(comments_string)\n",
    "            #foutc.write(str(outlist)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = namedtuple('row_raw', ['subreddit', 'submission_title', 'submitted_link', 'comments_link', 'short_name', 'imgurlhash'])\n",
    "\n",
    "with open(base_path+'imgposts_fasttext.train', \"wt\") as ft_train, open(base_path+'imgposts_fasttext.valid', \"wt\") as ft_valid, gzip.open(base_path+'safe_links_imgposts.gz', \"rt\") as f: \n",
    "    for i, line in enumerate(f):\n",
    "        line_list = eval(line)  # convert the str(list) to a list, don't do this in production, as this has the danger of code injection\n",
    "        r = row._make(line_list)\n",
    "        output = \"__label__\" + r.subreddit + \" \" + \" \".join(gensim.utils.simple_preprocess(r.submission_title)) + \"\\n\"\n",
    "        if i%10==0:\n",
    "            ft_valid.write(output)\n",
    "        else:\n",
    "            ft_train.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
