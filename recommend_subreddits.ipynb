{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit recommender\n",
    "This notebook demonstrates different experiments to suggest similar subreddits, based on a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import glob\n",
    "from collections import namedtuple\n",
    "import ml_metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this notebook, modify `base_path` to the location of `safe_links_imgposts.gz` from the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/mnt/marcel/\"\n",
    "corpus_path = base_path + \"subreddits/\"\n",
    "row = namedtuple('row_raw', ['subreddit', 'submission_title', 'submitted_link', 'comments_link', 'short_name', 'imgurlhash'])\n",
    "\n",
    "                \n",
    "def read_corpus_postquery():\n",
    "    with gzip.open(base_path+'safe_links_imgposts.gz', \"rt\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line_list = eval(line)  # convert the str(list) to a list, don't do this in production, as this has the danger of code injection\n",
    "            r = row._make(line_list)\n",
    "            subreddit = r.subreddit\n",
    "            if i%100==0:\n",
    "                subreddit += \"_eval_%s\" % i\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(r.submission_title), [subreddit])\n",
    "\n",
    "def read_corpus_subredditquery():\n",
    "    with gzip.open(base_path+'safe_links_imgposts.gz', \"rt\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line_list = eval(line)  # convert the str(list) to a list, don't do this in production, as this has the danger of code injection\n",
    "            r = row._make(line_list)\n",
    "            subreddit = r.subreddit\n",
    "            if i%10==0:\n",
    "                subreddit += \"_eval\"\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(r.submission_title), [subreddit])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 38min 34s, sys: 47min 41s, total: 2h 26min 15s\n",
      "Wall time: 1h 30min 5s\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map@1 0.00044827362286782005\n",
      "map@2 0.0006315630714703166\n",
      "map@3 0.0007286191985398288\n",
      "map@4 0.0008103139242026559\n",
      "map@5 0.0008639391800223578\n",
      "map@6 0.0009061830338907428\n",
      "map@7 0.0009414943072541626\n",
      "map@8 0.0009729153555860192\n",
      "map@9 0.0009950264636713994\n",
      "map@10 0.0009950264636713994\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by subreddit\n",
    "This model employes the doc2vec library to calculate vectors representing a subreddit. For evaluating the quality of similarities, 10% of all posts are treated as belonging to their `evaluation` subreddit. We then expect to see the highest similarity scores between an evaluation subreddit and its subreddit. For example, 10% of the posts of `funny` go into `funny_eval`, then by querying `funny_eval` we expect to retrieve `funny.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"model_subredditquery.p\"\n",
    "if os.path.isfile(model_filename):\n",
    "    model_subredditquery = pickle.load(open(model_filename, \"rb\"))\n",
    "else:\n",
    "    model_subredditquery = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=10, workers=16, window=5)\n",
    "    model_subredditquery.build_vocab(read_corpus_subredditquery())\n",
    "    %time model_subredditquery.train(read_corpus_subredditquery(), total_examples=model_subredditquery.corpus_count, epochs=10)\n",
    "    pickle.dump(model_subredditquery, open(model_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map@1 0.03901969205940156\n",
      "map@2 0.04378977782983874\n",
      "map@3 0.045303884181710115\n",
      "map@4 0.046133605785708395\n",
      "map@5 0.04671408552748602\n",
      "map@6 0.04706766623716372\n",
      "map@7 0.047311237173015316\n",
      "map@8 0.04754063079294425\n",
      "map@9 0.047728628716290265\n",
      "map@10 0.04787439941377703\n"
     ]
    }
   ],
   "source": [
    "actual = []\n",
    "predicted = []\n",
    "for k in model_subredditquery.docvecs.doctags.keys():\n",
    "    if \"_eval\" in k:\n",
    "        preds = model_subredditquery.docvecs.most_similar(k, topn=1000)\n",
    "        actual.append([k[0:k.index(\"_eval\")]])\n",
    "        predicted.append([p[0] for p in preds if \"_eval\" not in p[0]][0:10])\n",
    "        \n",
    "for i in range(1,11):\n",
    "    print(\"map@%s\" % i, ml_metrics.mapk(actual, predicted, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, about 4% of all evaluation subreddits are most similar to their orginal counter part. This evaluation is a bit unfair, as the majority of subreddits is very small, and thus both not often relevant for a user query (assuming a user is interested in big subreddits) and it might not have enough representative data to learn a good vector. To verify this is the case in the next evaluation we only take subreddits into consideration which consist of more than 100 posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map@1 0.5740922473012757\n",
      "map@2 0.6303565587176971\n",
      "map@3 0.6421328099443899\n",
      "map@4 0.647693817468106\n",
      "map@5 0.6511612692181877\n",
      "map@6 0.6536691745720205\n",
      "map@7 0.6554916896428182\n",
      "map@8 0.6566366029565245\n",
      "map@9 0.6575816107710121\n",
      "map@10 0.6582358469502728\n"
     ]
    }
   ],
   "source": [
    "actual = []\n",
    "predicted = []\n",
    "for k, v in model_subredditquery.docvecs.doctags.items():\n",
    "    if \"_eval\" in k and v.doc_count > 100:\n",
    "        preds = model_subredditquery.docvecs.most_similar(k, topn=1000)\n",
    "        true_tag = k[0:k.index(\"_eval\")]\n",
    "        actual.append([true_tag])\n",
    "        predictions = [p[0] for p in preds if \"_eval\" not in p[0]][0:10]\n",
    "        predicted.append(predictions)\n",
    "        if v.doc_count > 100000 and not true_tag in predictions:\n",
    "            print(true_tag, predictions)\n",
    "        \n",
    "for i in range(1,11):\n",
    "    print(\"map@%s\" % i, ml_metrics.mapk(actual, predicted, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates, that this method is suitable for retrieving similar subreddits, as in about 60% of the cases the correct one is retrieved. In the following cell you can query the model to retrieve similar subreddits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doctorwho_eval', 0.9095908999443054),\n",
       " ('DoctorWhumour', 0.8420202732086182),\n",
       " ('DoctorWhumour_eval', 0.797204852104187),\n",
       " ('gallifrey', 0.796032726764679),\n",
       " ('drwho', 0.7927993535995483),\n",
       " ('gallopfrey', 0.7127442359924316),\n",
       " ('doctorwhocirclejerk', 0.7006417512893677),\n",
       " ('wholock', 0.6978670954704285),\n",
       " ('Sherlock', 0.6900186538696289),\n",
       " ('Torchwood', 0.6789624691009521)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_subredditquery.docvecs.most_similar(\"doctorwho\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by post (or query string/keywords)\n",
    "We again use the doc2vec algorithm to compute vectors for documents. In doc2vec each document gets a tag and as shown above one can retrieve similar `tags`. So if we want to be able to query by a post, which can be seen as a query string/list of keywords, we need to give those its individual tag. Further down we demonstrate how to query by a list of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 52min 38s, sys: 54min 25s, total: 2h 47min 4s\n",
      "Wall time: 1h 43min 21s\n"
     ]
    }
   ],
   "source": [
    "model_filename = \"model_postquery.p\"\n",
    "if os.path.isfile(model_filename):\n",
    "    model_postquery = pickle.load(open(model_filename, \"rb\"))\n",
    "else:\n",
    "    model_postquery = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=10, workers=16, window=5)\n",
    "    model_postquery.build_vocab(read_corpus_postquery())\n",
    "    %time model_postquery.train(read_corpus_postquery(), total_examples=model_postquery.corpus_count, epochs=10)\n",
    "    pickle.dump(model_postquery, open(model_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for td in read_corpus_postquery():\n",
    "    if \"_eval_\" in td.tags[0]:\n",
    "        true_tag = td.tags[0][0:td.tags[0].index(\"_eval_\")]\n",
    "        actual.append([true_tag])\n",
    "        preds = model_postquery.docvecs.most_similar(td.tags[0], topn=5000)\n",
    "        predicted.append([p[0] for p in preds if \"_eval_\" not in p[0]][0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map@1 0.002722063406548934\n",
      "map@2 0.0036861275297016814\n",
      "map@3 0.004159567219413949\n",
      "map@4 0.004417338375337143\n",
      "map@5 0.004594684930612301\n",
      "map@6 0.004715837373896203\n",
      "map@7 0.004800901855350857\n",
      "map@8 0.00486244471882752\n",
      "map@9 0.004912280475639336\n",
      "map@10 0.004964350249135822\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    print(\"map@%s\" % i, ml_metrics.mapk(actual, predicted, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('420vendorshop', 0.8758198022842407),\n",
       " ('gtaonlinecrews_eval_38377400', 0.8756065368652344),\n",
       " ('InfinitySuns', 0.8755130767822266),\n",
       " ('nobleboners', 0.8748517036437988),\n",
       " ('DowniesAndDogshit', 0.8743728399276733),\n",
       " ('AdrianaLimaArmpits', 0.8740894794464111),\n",
       " ('WE_BUY_DVDs', 0.8730192184448242),\n",
       " ('Beefeaters', 0.8719935417175293),\n",
       " ('conspire', 0.8703101873397827),\n",
       " ('Anxietyquotes', 0.8697609305381775)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_vector = model_postquery.infer_vector(['sonic', 'screwdriver'])\n",
    "model_postquery.docvecs.most_similar([inferred_vector], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from both the MAP scores, as well as the inferred vectors, this is not working at all. The fastText model yields significantly better results for keyword queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
